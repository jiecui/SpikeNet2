{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb93ebaf-bfad-46f2-9d68-281f1db74c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mat73\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5272f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming signal is your original signal stored as a numpy array\n",
    "# signal = np.array([...])\n",
    "sampling_rate = 128\n",
    "time_step = 8\n",
    "window_length = 3  # in seconds\n",
    "num_points = sampling_rate * window_length\n",
    "threshold = 0.4  # 0.75 0.5 0.4\n",
    "cluster_len = 2  # 10 8 5 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d05021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_mine(df):\n",
    "    # Correcting the column name and identifying false positives\n",
    "    column_name = df.columns[0]\n",
    "    false_positives = df[df[column_name] > threshold]\n",
    "\n",
    "    # Display the number of false positives and their first few rows\n",
    "    num_false_positives = len(false_positives)\n",
    "    false_positives.head(), num_false_positives\n",
    "\n",
    "    # Find clusters of false positives\n",
    "    clusters = []\n",
    "    current_cluster = []\n",
    "\n",
    "    # Iterate over the false positives\n",
    "    for idx in false_positives.index:\n",
    "        # If current_cluster is empty or the current index is consecutive to the last index in current_cluster\n",
    "        if not current_cluster or idx == current_cluster[-1] + 1:\n",
    "            current_cluster.append(idx)\n",
    "        else:\n",
    "            # If the current index is not consecutive, check if the current cluster is valid (has at least 8 false positives)\n",
    "            if len(current_cluster) >= cluster_len:\n",
    "                clusters.append(current_cluster)\n",
    "            # Reset current_cluster and start a new one\n",
    "            current_cluster = [idx]\n",
    "\n",
    "    # Check for the last cluster\n",
    "    if len(current_cluster) >= cluster_len:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    # Display the number of clusters and the indices of the first few clusters\n",
    "    num_clusters = len(clusters)\n",
    "    # Extract the index of the highest prediction value from each cluster\n",
    "    highest_prediction_indices = []\n",
    "    bonobo_idx = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        max_index = df.loc[cluster][column_name].idxmax()\n",
    "        highest_prediction_indices.append(max_index)\n",
    "\n",
    "    filtered_indices = []\n",
    "    for idx in sorted(highest_prediction_indices):\n",
    "        if not filtered_indices or (idx - filtered_indices[-1] > num_points):\n",
    "            filtered_indices.append(idx)\n",
    "        else:\n",
    "            if idx - filtered_indices[-1] <= num_points:\n",
    "                last_idx = filtered_indices.pop()  # Store the popped value if needed\n",
    "    return filtered_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_controls = os.path.join(\"your_path/SpikeNet2/controlset.csv\")\n",
    "controls = pd.read_csv(path_controls)\n",
    "# train_controls = controls[controls['Mode']=='Train']\n",
    "train_controls = controls\n",
    "\n",
    "for eeg_file in tqdm(train_controls.EEG_index):\n",
    "    df_path = \"your_path/SpikeNet2/Models/SpikeNet2/con_hardmine/\" + eeg_file + \".csv\"\n",
    "    signal_path = \"your_path/Bonobo_data/\" + eeg_file + \".mat\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    filtered_indices = hard_mine(df)\n",
    "    signal = mat73.loadmat(signal_path)[\"data\"]  # 128 sample rate\n",
    "    signal = signal.transpose(1, 0)\n",
    "    start_times = filtered_indices  # 16 sample rate\n",
    "\n",
    "    # Calculate start indices\n",
    "    start_indices = [int(time * time_step) for time in start_times]\n",
    "\n",
    "    # Extract signal segments\n",
    "    segments = [signal[start : start + num_points] for start in start_indices]\n",
    "    for idx, seg in zip(start_indices, segments):\n",
    "        path = str(\n",
    "            \"your_path/SpikeNet2/Models/SpikeNet2/hardmine_npy_round2/\"\n",
    "            + eeg_file\n",
    "            + \"_\"\n",
    "            + str(idx)\n",
    "            + \".npy\"\n",
    "        )\n",
    "        seg = seg.transpose(1, 0)\n",
    "        # print(seg.shape)\n",
    "        np.save(path, seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3945622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_directory(directory_path):\n",
    "    items = os.listdir(directory_path)\n",
    "\n",
    "    return len(\n",
    "        [item for item in items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122374b-4cb7-48a9-be45-cf4ade37dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_files_in_directory(\"your_path/SpikeNet2/Models/SpikeNet2/hardmine_npy_round2\")\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv(\"your_path/SpikeNet2/hard_mining_round1.csv\")\n",
    "\n",
    "# Get the names of all .npy files in the folder\n",
    "npy_files = [\n",
    "    f\n",
    "    for f in os.listdir(\"your_path/SpikeNet2/Models/SpikeNet2/hardmine_npy_round2\")\n",
    "    if f.endswith(\".npy\")\n",
    "]\n",
    "\n",
    "# Extract file name part\n",
    "event_files = [f[:-4] for f in npy_files]  # remove.npy\n",
    "eeg_files = [\n",
    "    f.split(\"_\")[:-1] for f in event_files\n",
    "]  # # Use '_' as delimiter and remove the last part\n",
    "eeg_files = [\"_\".join(f) for f in eeg_files]\n",
    "# Create a new DataFrame to store filenames and other information\n",
    "new_data = {\n",
    "    \"event_file\": event_files,\n",
    "    \"eeg_file\": eeg_files,\n",
    "    \"total_votes_received\": [3] * len(event_files),\n",
    "    \"fraction_of_yes\": [0] * len(event_files),\n",
    "    \"Mode\": [\"Train\"] * len(event_files),\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Add new data to the original DataFrame\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "df.to_csv(\"your_path/SpikeNet2/hard_mining_round2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpikeNet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
